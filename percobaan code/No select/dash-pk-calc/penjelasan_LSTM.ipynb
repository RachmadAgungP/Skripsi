{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Membaca data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def readData(filename):\n",
    "    data = pd.read_csv('%s'%filename)\n",
    "\n",
    "    data = data[['Date','Close']]\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "    data1 = data[['Date','Close']]\n",
    "    data1['Date'] = pd.to_datetime(data['Date'])\n",
    "    s1 = data1.values.tolist()\n",
    "    # data1.insert(0, \"subject_index\",0) \n",
    "    # ex_csv1 = pd.DataFrame(data1)\n",
    "    # ex_csv1.to_csv(\"SMGRW.csv\")\n",
    "\n",
    "    data['Date'] =  data['Date'].dt.strftime(\"%Y%m%d\").astype(int)\n",
    "\n",
    "    s = data.values.tolist()\n",
    "    training_data  = np.array(s)\n",
    "\n",
    "    min_ex = np.amin(training_data, axis=0)\n",
    "    max_ex = np.amax(training_data, axis=0)\n",
    "    original_data = np.copy(training_data)\n",
    "    training_data -= min_ex\n",
    "    training_data /= max_ex\n",
    "    tbl_data = pd.DataFrame(data=training_data,columns=[\"date\",\"x(close)\"])\n",
    "    min_data = pd.DataFrame(data=min_ex,columns=[0]).T\n",
    "    max_data = pd.DataFrame(data=max_ex,columns=[1]).T\n",
    "    frame = [min_data,max_data]\n",
    "    tbl_min_max = pd.concat(frame)\n",
    "    tbl_min_max = tbl_min_max.rename(columns={0:\"data\",1:\"x(close)\"},index={0:\"max\",1:\"min\"})\n",
    "    return (training_data, max_ex, min_ex, original_data,tbl_data,tbl_min_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "- tabel data normalisasi -\n           date  x(close)\n0  0.000000e+00  0.030303\n1  4.965213e-08  0.020202\n2  1.986085e-07  0.026263\n3  2.979128e-07  0.018182\n4  3.475649e-07  0.000000\n5  3.972170e-07  0.016162\n6  5.461734e-07  0.026263\n7  5.958256e-07  0.022222\n8  6.454777e-07  0.024242\n9  6.951298e-07  0.000000\n\n-- tabel data min max --\n           data  x(close)\nmax  20140109.0   12000.0\nmin  20140123.0   12375.0\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "DataSahamStr = 'ujicoba.csv'\n",
    "\n",
    "data = readData(DataSahamStr)\n",
    "tbl_data = data[4]\n",
    "tbl_min_max = data[5]\n",
    "\n",
    "print (\"- tabel data normalisasi -\")\n",
    "print (tbl_data)\n",
    "print ()\n",
    "print (\"-- tabel data min max --\")\n",
    "print (tbl_min_max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inisial variable untuk input LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Panjang memory(sequenceLength) adalah 5\nbanyak epoch adalah 1\nLearnig rate adalah 0.1\n"
    }
   ],
   "source": [
    "# I_SequenceLength = int(input(\"masukkan panjang memory: \"))\n",
    "I_SequenceLength = 5\n",
    "print (\"Panjang memory(sequenceLength) adalah %s\" %I_SequenceLength)\n",
    "sequenceLength = I_SequenceLength\n",
    "\n",
    "# numEpochs = int(input(\"masukkan banyak epoch : \"))\n",
    "numEpochs = 1\n",
    "print (\"banyak epoch adalah %s\" %numEpochs)\n",
    "\n",
    "data_saham_Normalisasi = data[0]\n",
    "\n",
    "# rate = float(input(\"masukkan learning rate : \"))\n",
    "rate = 0.1\n",
    "print (\"Learnig rate adalah %s\" %rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persiapan Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# menambhakan data 1 atau bias \n",
    "data_saham_Normalisasi = np.concatenate((np.ones((data_saham_Normalisasi.shape[0], 1)), data_saham_Normalisasi), axis=1)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "banyak data training :  7\n   bias          date  x(close)\n0   1.0  0.000000e+00  0.030303\n1   1.0  4.965213e-08  0.020202\n2   1.0  1.986085e-07  0.026263\n3   1.0  2.979128e-07  0.018182\n4   1.0  3.475649e-07  0.000000\n5   1.0  3.972170e-07  0.016162\n6   1.0  5.461734e-07  0.026263\n10\nbanyak data testing :  5\n"
    }
   ],
   "source": [
    "def bagi_data(skenario,data_sa):\n",
    "    if (skenario == 1):\n",
    "        trainingData = data_sa[:-250]\n",
    "        forecastData = data_sa[250:500]\n",
    "    elif (skenario == 2):\n",
    "        trainingData = data_sa[:-500]\n",
    "        forecastData = data_sa[500:1000]\n",
    "    elif (skenario == 5):\n",
    "        trainingData = data_sa[:7]\n",
    "        forecastData = data_sa[5:10]\n",
    "    else :\n",
    "        trainingData = data_sa[:-1000]\n",
    "        forecastData = data_sa[500:-500]\n",
    "    return (trainingData, forecastData)\n",
    "\n",
    "# skenarioI = int(input(\"masukkan skenario pilihan : \"))\n",
    "skenarioI = 5\n",
    "\n",
    "# sebelum digabungkan 1 = [0.00000000e+00 4.32551320e-01]\n",
    "import numpy as np\n",
    "# setelah digabungkan 1 = [1.00000000e+00 0.00000000e+00 4.32551320e-01]\n",
    "# pembagian skenario\n",
    "skenarioP = bagi_data(skenarioI,data_saham_Normalisasi)\n",
    "print (\"banyak data training : \",len(skenarioP[0]))\n",
    "training_Data = skenarioP[0]\n",
    "tbl_data_training = pd.DataFrame(data=training_Data,columns=[\"bias\",\"date\",\"x(close)\"])\n",
    "print (tbl_data_training)\n",
    "print (data_saham_Normalisasi.shape[0])\n",
    "print (\"banyak data testing : \",len(skenarioP[1]))\n",
    "testing_Data = skenarioP[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator untuk memotong data sesuai squencial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sequenceProducer(trainingData, sequenceLength):\n",
    "    indices = [i for i in range(0, trainingData.shape[0] - sequenceLength + 1, sequenceLength)]\n",
    "    #random.shuffle(indices)\n",
    "    for index in indices:\n",
    "        yield trainingData[index:index + sequenceLength+2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a = []\n",
    "for r in range(1,100):\n",
    "    a.append(r)\n",
    "\n",
    "b = sequenceProducer(a, 5)\n",
    "for t in range(1,20):\n",
    "    print (next(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecastSequenceProducer(trainingData, sequenceLength):\n",
    "    for i in range(trainingData.shape[0] - sequenceLength + 1):\n",
    "        yield trainingData[i:i + sequenceLength]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c = []\n",
    "for r in range(1,100):\n",
    "    c.append(r)\n",
    "\n",
    "d = forecastSequenceProducer(c, 5)\n",
    "for t in range(1,20):\n",
    "    print (next(d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Ukuran Input  3\nUkuran Output  1\nUkuran bobot  4\n-------------- matriks bobot ------------\n       bias      date  x(close)  h(close)\nC -0.245714  0.850361  0.029262  0.184398\ni  0.868020  0.860430 -0.379581  0.079507\nf -0.206444 -0.248562 -0.085253  0.251126\no  0.842874 -0.324206  0.907723 -0.593739\n"
    }
   ],
   "source": [
    "inputSize = data_saham_Normalisasi.shape[1]\n",
    "numCells = data_saham_Normalisasi.shape[1]-2\n",
    "W = [[-0.245714286\t,0.850360602\t,0.029262045\t,0.184398087]\n",
    "        ,[0.868020398\t,0.860429754\t,-0.379580925\t,0.079506914]\n",
    "        ,[-0.206444161\t,-0.24856166\t,-0.085253247\t,0.25112624\t]\n",
    "        ,[0.842874383\t,-0.324206065\t,0.907722829\t,-0.593738792]]\n",
    "\n",
    "mtrx_w = pd.DataFrame(data=W,index=[\"C\",\"i\",\"f\",\"o\"], columns=[\"bias\",\"date\",\"x(close)\",\"h(close)\"])\n",
    "\n",
    "print (\"Ukuran Input \",inputSize)\n",
    "print (\"Ukuran Output \",numCells)\n",
    "print (\"Ukuran bobot \",len(W))\n",
    "print (\"-------------- matriks bobot ------------\")\n",
    "print (mtrx_w)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layer (block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "class LSTMCell: \n",
    "    # numCells = ukuran penampungan I,o,z,dll\n",
    "    # Size is the dimensionality of the input vector\n",
    "    def __init__(self, inputSize, numCells, W):\n",
    "        self.inputSize = inputSize\n",
    "        self.numCells = numCells\n",
    "\n",
    "        # Randomly initialise the weight matrix\n",
    "        # self.W = np.random.random((4 * numCells, inputSize + numCells)) * 2 \\\n",
    "        #                 - np.ones((4 * numCells, inputSize + numCells))\n",
    "        \n",
    "        self.W = W\n",
    "        \n",
    "        W = pd.DataFrame(self.W)\n",
    "        #bobot disimpan .csv\n",
    "        W.to_csv(\"P_W.csv\",header=False,index=False) \n",
    "        #pemanggilan .csv\n",
    "        # weight = pd.read_csv('P_W.csv')\n",
    "        # weight = weight.values.tolist()\n",
    "        # weighting = np.array(weight)\n",
    "        # self.W = weighting\n",
    "                    \n",
    "        self.h = []\n",
    "        self.C = []\n",
    "        self.C_bar = []\n",
    "        self.i = []\n",
    "        self.f = []\n",
    "        self.o = []\n",
    "\n",
    "        self.I = []\n",
    "        self.z = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMCell(inputSize, numCells,W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FORWARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is the input vector (including bias term), returns output h\n",
    "def forwardStep(x):\n",
    "    \n",
    "    I = np.concatenate((x, lstm.h[-1])) #mengabungkan\n",
    "    lstm.I.append(I) \n",
    "    z = np.dot(lstm.W, I)\n",
    "    lstm.z.append(z)\n",
    "    # Compute the candidate value vector\n",
    "    C_bar = np.tanh(z[0:lstm.numCells])\n",
    "    lstm.C_bar.append(C_bar)\n",
    "    # Compute input gate vector\n",
    "    i = sigmoid(z[lstm.numCells:lstm.numCells * 2])\n",
    "    lstm.i.append(i)\n",
    "    # Compute forget gate vector\n",
    "    f = sigmoid(z[lstm.numCells * 2:lstm.numCells * 3])\n",
    "    lstm.f.append(f)\n",
    "    # Compute the output gate vector\n",
    "    o = sigmoid(z[lstm.numCells * 3:])\n",
    "    lstm.o.append(o)\n",
    "    # Compute the new state vector as the elements of the old state allowed\n",
    "    # through by the forget gate, plus the candidate values allowed through\n",
    "    # by the input gate\n",
    "    C = np.multiply(f, lstm.C[-1]) + np.multiply(i, C_bar)\n",
    "    lstm.C.append(C)\n",
    "    # Compute the new output\n",
    "    h = np.multiply(o, np.tanh(C))\n",
    "    lstm.h.append(h)\n",
    "    \n",
    "    return (h,C,o,f,i,C_bar,z,I,lstm.W)\n",
    "# x = trainingSequences (data training)\n",
    "def forwardPass(x):\n",
    "    \n",
    "    numCells = lstm.numCells \n",
    "    \n",
    "    lstm.h.append(np.zeros(numCells)) # initial output is empty\n",
    "    \n",
    "    lstm.C.append(np.zeros(numCells)) # initial state is empty\n",
    "    \n",
    "    lstm.C_bar.append(np.zeros(numCells)) # this and the following\n",
    "    \n",
    "    # empty arrays make the indexing follow the indexing in papers\n",
    "    lstm.i.append(np.zeros(numCells)) \n",
    "    lstm.f.append(np.zeros(numCells)) \n",
    "    lstm.o.append(np.zeros(numCells)) \n",
    "    lstm.I.append(np.zeros(numCells)) \n",
    "    lstm.z.append(np.zeros(numCells)) \n",
    "\n",
    "    O_W= []\n",
    "    O_I= []\n",
    "    O_z= []\n",
    "    O_c= []\n",
    "    O_o= []\n",
    "    O_f= []\n",
    "    O_in= []\n",
    "    O_c_bar= []\n",
    "    O_h = []\n",
    "    \n",
    "    for x_t in x:\n",
    "        w = forwardStep(x_t)\n",
    "        O_W.append(w[8])    \n",
    "        O_I.append(w[7])    \n",
    "        O_z.append(w[6])    \n",
    "        O_c.append(w[1])    \n",
    "        O_o.append(w[2])    \n",
    "        O_f.append(w[3])    \n",
    "        O_in.append(w[4])   \n",
    "        O_c_bar.append(w[5])\n",
    "        O_h.append(w[0])    \n",
    "    \n",
    "    # print(O_h)\n",
    "    return (O_I,O_z,O_c,O_o,O_f,O_in,O_c_bar,O_h,O_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backwardStep(t, dE_dh_t, dE_dc_tplus1):\n",
    "    dE_do_t = np.multiply(dE_dh_t, np.tanh(lstm.C[t]))\n",
    "    #print(\"-----------------------------------------\")\n",
    "    dE_dc_t = dE_dc_tplus1 + np.multiply(np.multiply(dE_dh_t, lstm.o[t]), (np.ones(lstm.numCells) - np.square(np.tanh(lstm.C[t]))))\n",
    "    dE_di_t = np.multiply(dE_dc_t, lstm.C_bar[t])\n",
    "    dE_dcbar_t = np.multiply(dE_dc_t, lstm.i[t])\n",
    "    dE_df_t = np.multiply(dE_dc_t, lstm.C[t - 1])\n",
    "    dE_dc_tminus1 = np.multiply(dE_dc_t, lstm.f[t])\n",
    "    \n",
    "    dE_dzcbar_t = np.multiply(dE_dcbar_t, (np.ones(lstm.numCells) - np.square(np.tanh(lstm.z[t][0:lstm.numCells]))))\n",
    "    dE_dzi_t = np.multiply(np.multiply(dE_di_t, lstm.i[t]), (np.ones(lstm.numCells) - lstm.i[t]))\n",
    "    dE_dzf_t = np.multiply(np.multiply(dE_df_t, lstm.f[t]), (np.ones(lstm.numCells) - lstm.f[t]))\n",
    "    dE_dzo_t = np.multiply(np.multiply(dE_do_t, lstm.o[t]), (np.ones(lstm.numCells) - lstm.o[t]))\n",
    "    dE_dz_t = np.concatenate((dE_dzcbar_t, dE_dzi_t, dE_dzf_t, dE_dzo_t))\n",
    "    dE_dI_t = np.dot(np.transpose(lstm.W), dE_dz_t)\n",
    "    dE_dh_tminus1 = dE_dI_t[lstm.inputSize:]\n",
    "    dE_dz_t.shape = (len(dE_dz_t), 1)\n",
    "    lstm.I[t].shape = (len(lstm.I[t]), 1)\n",
    "    dE_dW_t = np.dot(dE_dz_t, np.transpose(lstm.I[t])) # this one is confusing cos it says X_t instead of I_t, but there is no matrix or vector X,\n",
    "    # and the matrix dimensions are correct if we use I instead\n",
    "    # r = pd.DataFrame(dE_dW_t)\n",
    "    # r.to_csv('dE_dW_t%s.csv'%t)\n",
    "    return (dE_dW_t, dE_dh_tminus1, dE_dc_tminus1, dE_do_t, dE_dc_t, dE_di_t, dE_dcbar_t,dE_df_t,dE_dzcbar_t,dE_dzi_t,dE_dzf_t,dE_dzo_t,dE_dz_t,dE_dI_t)\n",
    "# Back propagation through time, returns the error and the gradient for this sequence\n",
    "# (should I give this the sequence x1,x2,... so that this method is tied\n",
    "# to the sequence?)\n",
    "def BPTT(y):\n",
    "    numTimePeriods = len(y)\n",
    "    dE_dW = 0 \n",
    "    dE_dh_t = 0\n",
    "    dE_dc_t = 0\n",
    "    E = 0.0\n",
    "    discount = 1.0\n",
    "    dE_dW_t_list= []\n",
    "    dE_dh_tminus1_list= []\n",
    "    dE_dc_tminus1_list= []\n",
    "    dE_do_t_list= []\n",
    "    dE_dc_t_list= []\n",
    "    dE_di_t_list= []\n",
    "    dE_dcbar_t_list= []\n",
    "    dE_df_t_list= []\n",
    "    dE_dzcbar_t_list= []\n",
    "    dE_dzi_t_list= []\n",
    "    dE_dzf_t_list= []\n",
    "    dE_dzo_t_list= []\n",
    "    dE_dz_t_list= []\n",
    "    dE_dI_t_list= []\n",
    "\n",
    "    dE_dh_tplus1_list= []\n",
    "    dE_dc_tplus1_list= []\n",
    "\n",
    "    dE_dW_list = []\n",
    "    error_list = []  \n",
    "    for i in range(numTimePeriods):\n",
    "        index = numTimePeriods - i\n",
    "        E = E + 0.5 * np.sum(np.absolute(lstm.h[index] - y[index - 1])) # This is the error/loss vector for this sequence\n",
    "        error_list.append(E)\n",
    "        # The gradient is just 1 or -1, depending on whether h is\n",
    "        # less than or greater than y\n",
    "        lessThan = np.less(lstm.h[index], y[index - 1])\n",
    "        greaterThan = np.greater(lstm.h[index], y[index - 1])\n",
    "        dE_dh_t -= 0.5 * lessThan\n",
    "        dE_dh_t += 0.5 * greaterThan\n",
    "        dE_dh_tplus1_list.append(dE_dh_t)\n",
    "        dE_dc_tplus1_list.append(dE_dc_t)\n",
    "        #dE_dh_t += self.h[index] - y[index - 1] # This is the error gradient for this sequence\n",
    "        result = backwardStep(index, dE_dh_t, dE_dc_t) \n",
    "        dE_dW = dE_dW + result[0] # dE_dW_t\n",
    "        dE_dW_list.append(dE_dW)\n",
    "\n",
    "        dE_dh_t = result[1]\n",
    "        dE_dc_t = result[2]\n",
    "\n",
    "        dE_dW_t_list.append(result[0])\n",
    "        dE_dh_tminus1_list.append(dE_dh_t)\n",
    "        dE_dc_tminus1_list.append(dE_dc_t)\n",
    "        dE_do_t_list.append(result[3])\n",
    "        dE_dc_t_list.append(result[4])\n",
    "        dE_di_t_list.append(result[5])\n",
    "        dE_dcbar_t_list.append(result[6])\n",
    "        dE_df_t_list.append(result[7])\n",
    "        dE_dzcbar_t_list.append(result[8])\n",
    "        dE_dzi_t_list.append(result[9])\n",
    "        dE_dzf_t_list.append(result[10])\n",
    "        dE_dzo_t_list.append(result[11])\n",
    "        dE_dz_t_list.append(result[12])\n",
    "        dE_dI_t_list.append(result[13])\n",
    "        # discount *= 0.99\n",
    "        \n",
    "    return (E / (numTimePeriods), dE_dW, dE_dW_t_list, dE_dh_tminus1_list, dE_dc_tminus1_list, dE_do_t_list, dE_dc_t_list, dE_di_t_list, dE_dcbar_t_list, dE_df_t_list, dE_dzcbar_t_list, dE_dzi_t_list, dE_dzf_t_list, dE_dzo_t_list, dE_dz_t_list, dE_dI_t_list, dE_dh_tplus1_list, dE_dc_tplus1_list, dE_dW_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_hitung_manual(jenis_proses,data_t,kon):\n",
    "    if jenis_proses == \"forward\":\n",
    "        print (\"PROSES FORWARD\")\n",
    "        if (kon == \"I\" or kon == \"Z\" or kon == \"W\"):\n",
    "            view = pd.DataFrame(data=data_t[kon].tolist(),columns=[\"C\",\"i\",\"f\",\"o\"]).T\n",
    "            return(view)\n",
    "        else:\n",
    "            view = pd.DataFrame(data=data_t[kon].tolist(),columns=[i]).T\n",
    "            return(view)\n",
    "    elif jenis_proses == \"backward\":\n",
    "        print (\"PROSES BACKWARD\")\n",
    "        if kon == \"dE_dW_t\" or kon == \"dE_dW_list\":\n",
    "            view = pd.DataFrame(data=data_t[kon].tolist(),columns=[\"C\",\"i\",\"f\",\"o\"]).T\n",
    "            return(view)\n",
    "        elif kon == \"dE_dz_t\":\n",
    "            view = pd.DataFrame(data=data_t[kon].tolist(),columns=[\"dE_dzcbar_t\", \"dE_dzi_t\", \"dE_dzf_t\", \"dE_dzo_t\"]).T\n",
    "            return(view)\n",
    "        elif kon == \"dE_dI_t\":\n",
    "            view = pd.DataFrame(data=data_t[kon].tolist(),columns=[\"bias\", \"date\", \"x(close)\", \"h(Close)\"]).T\n",
    "            return(view)\n",
    "        else:\n",
    "            view = pd.DataFrame(data=data_t[kon].tolist(),columns=[kon]).T\n",
    "            return(view)\n",
    "    else:\n",
    "        view = pd.DataFrame(data=data_t[kon].tolist(),columns=[kon]).T\n",
    "        return(view)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tampung_hitung_manual(jenis_proses,data):\n",
    "    if jenis_proses == \"forward\": \n",
    "        var_hidden_layer = data\n",
    "        list_table_hitung_str = [\"I\",\"Z\",\"C\",\"O\",\"F\",\"I_in\",\"C_bar\",\"h\",\"W\"]\n",
    "    elif jenis_proses == \"backward\":\n",
    "        var_hidden_layer = data\n",
    "        list_table_hitung_str = [\"dE_dW_t\", \"dE_dh_tminus1\", \"dE_dc_tminus1\", \"dE_do_t\", \"dE_dc_t\", \"dE_di_t\", \"dE_dcbar_t\", \"dE_df_t\",\"dE_dzcbar_t\",\"dE_dzi_t\",\"dE_dzf_t\",\"dE_dzo_t\",\"dE_dz_t\",\"dE_dI_t\", \"dE_dh_tplus1\", \"dE_dc_tplus1\",\"dE_dW\"]\n",
    "    else:\n",
    "        var_hidden_layer = data\n",
    "        list_table_hitung_str = [\"W\"]\n",
    "\n",
    "    data_full = {}\n",
    "    data_perhitungan = pd.DataFrame(data_full) \n",
    "    vel_hidden_layer = []\n",
    "    count = 0\n",
    "    for i in var_hidden_layer:\n",
    "        data_perhitungan.insert(count, list_table_hitung_str[count],np.transpose(np.transpose(i)).tolist(), True) \n",
    "        count += 1\n",
    "    return data_perhitungan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "forecast_h ([array([1.        , 0.        , 0.03030303, 0.        ]), array([ 1.00000000e+00,  4.96521297e-08,  2.02020202e-02, -1.17651092e-01]), array([ 1.00000000e+00,  1.98608519e-07,  2.62626263e-02, -1.80354973e-01]), array([ 1.00000000e+00,  2.97912778e-07,  1.81818182e-02, -2.12725368e-01]), array([ 1.00000000e+00,  3.47564908e-07,  0.00000000e+00, -2.28708352e-01]), array([ 1.00000000e+00,  3.97217038e-07,  1.61616162e-02, -2.36390528e-01])], [array([-0.24482756,  0.85651795, -0.20902759,  0.87038114]), array([-0.26681773,  0.85099806, -0.23771174,  0.93106622]), array([-0.27820273,  0.84371231, -0.25397505,  0.97379725]), array([-0.28440815,  0.84420605, -0.26141522,  0.98568164]), array([-0.28788737,  0.8498368 , -0.26387892,  0.97866729]), array([-0.28883099,  0.84309142, -0.26718595,  0.99789875])], [array([-0.16849919]), array([-0.25694819]), array([-0.30191084]), array([-0.32504204]), array([-0.3374834]), array([-0.34282462])], [array([0.704825]), array([0.71729155]), array([0.72587572]), array([0.72823412]), array([0.7268437]), array([0.73064525])], [array([0.44793254]), array([0.44085033]), array([0.43684535]), array([0.43501585]), array([0.43441042]), array([0.43359807])], [array([0.70193264]), array([0.70077647]), array([0.69924649]), array([0.69935032]), array([0.70053291]), array([0.6991159])], [array([-0.24005037]), array([-0.26066125]), array([-0.27124085]), array([-0.27697999]), array([-0.28018919]), array([-0.2810585])], [array([-0.11765109]), array([-0.18035497]), array([-0.21272537]), array([-0.22870835]), array([-0.23639053]), array([-0.24111056])], [[[-0.245714286, 0.850360602, 0.029262045, 0.184398087], [0.868020398, 0.860429754, -0.379580925, 0.079506914], [-0.206444161, -0.24856166, -0.085253247, 0.25112624], [0.842874383, -0.324206065, 0.907722829, -0.593738792]], [[-0.245714286, 0.850360602, 0.029262045, 0.184398087], [0.868020398, 0.860429754, -0.379580925, 0.079506914], [-0.206444161, -0.24856166, -0.085253247, 0.25112624], [0.842874383, -0.324206065, 0.907722829, -0.593738792]], [[-0.245714286, 0.850360602, 0.029262045, 0.184398087], [0.868020398, 0.860429754, -0.379580925, 0.079506914], [-0.206444161, -0.24856166, -0.085253247, 0.25112624], [0.842874383, -0.324206065, 0.907722829, -0.593738792]], [[-0.245714286, 0.850360602, 0.029262045, 0.184398087], [0.868020398, 0.860429754, -0.379580925, 0.079506914], [-0.206444161, -0.24856166, -0.085253247, 0.25112624], [0.842874383, -0.324206065, 0.907722829, -0.593738792]], [[-0.245714286, 0.850360602, 0.029262045, 0.184398087], [0.868020398, 0.860429754, -0.379580925, 0.079506914], [-0.206444161, -0.24856166, -0.085253247, 0.25112624], [0.842874383, -0.324206065, 0.907722829, -0.593738792]], [[-0.245714286, 0.850360602, 0.029262045, 0.184398087], [0.868020398, 0.860429754, -0.379580925, 0.079506914], [-0.206444161, -0.24856166, -0.085253247, 0.25112624], [0.842874383, -0.324206065, 0.907722829, -0.593738792]]])\nrrr 0.11033429865897926 [[-2.22725522e+00 -4.12514401e-07 -4.40628486e-02  3.30399076e-01]\n [ 1.91161431e-01  3.71201544e-08  3.69875737e-03 -2.94481145e-02]\n [ 1.78829964e-01  4.69942165e-08  2.85627635e-03 -3.53566233e-02]\n [ 1.87126342e-01  4.39168322e-08  3.27136691e-03 -3.28762176e-02]]\nE 0.11033429865897926\n          0         1         2         3\n0 -0.245714  0.850361  0.029262  0.184398\n1  0.868020  0.860430 -0.379581  0.079507\n2 -0.206444 -0.248562 -0.085253  0.251126\n3  0.842874 -0.324206  0.907723 -0.593739\nEpoch 0 error: 0.11033429865897926\nPROSES FORWARD\n          0             1             2             3             4  \\\nC  1.000000  1.000000e+00  1.000000e+00  1.000000e+00  1.000000e+00   \ni  0.000000  4.965213e-08  1.986085e-07  2.979128e-07  3.475649e-07   \nf  0.030303  2.020202e-02  2.626263e-02  1.818182e-02  0.000000e+00   \no  0.000000 -1.176511e-01 -1.803550e-01 -2.127254e-01 -2.287084e-01   \n\n              5  \nC  1.000000e+00  \ni  3.972170e-07  \nf  1.616162e-02  \no -2.363905e-01  \nPROSES BACKWARD\n                      0         1         2        3         4         5\ndE_dc_tminus1 -0.141153 -0.216506 -0.256395 -0.27938 -0.295964 -0.313223\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "adaptiveLearningRate = rate\n",
    "\n",
    "data_perhitungan_training_csv = {}\n",
    "data_perhitungan_training = pd.DataFrame(data_perhitungan_training_csv) # df \n",
    "\n",
    "data_perhitungan_training_BPTT_csv = {}\n",
    "data_perhitungan_training_BPTT = pd.DataFrame(data_perhitungan_training_BPTT_csv) # df \n",
    "\n",
    "data_perhitungan_training_optimasi_csv = {}\n",
    "data_perhitungan_training_optimasi = pd.DataFrame(data_perhitungan_training_optimasi_csv) # df \n",
    "\n",
    "for epoch in range(numEpochs):\n",
    "    trainingSequences = sequenceProducer(training_Data, sequenceLength) #data training \n",
    "    epochError = 0.0\n",
    "    counter = 0\n",
    "    for sequence in trainingSequences:\n",
    "        counter += 1\n",
    "        # proses forward\n",
    "        forecast_h = forwardPass(sequence[:-1])\n",
    "        print (\"forecast_h\",forecast_h)\n",
    "        froward = [forecast_h[0],forecast_h[1],forecast_h[2],forecast_h[3],forecast_h[4],forecast_h[5],forecast_h[6],forecast_h[7],forecast_h[8]]\n",
    "        # melihat hasil perhitungan secara detail \n",
    "        data_perhitungan_new_training = tampung_hitung_manual(\"forward\",froward)\n",
    "\n",
    "        data_perhitungan_training = pd.concat([data_perhitungan_new_training, data_perhitungan_training]).reset_index(drop = True) \n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # proses backward \n",
    "        result = BPTT(sequence[1:,2:])\n",
    "        print (\"rrr\",result[0],result[1])\n",
    "        backward = [result[2],result[3],result[4],result[5],result[6],result[7],result[8],result[9],result[10],result[11],result[12],result[13],result[14],result[15],result[16],result[17],result[18]]\n",
    "        # melihat hasil perhitungan secara detail \n",
    "        data_perhitungan_new_training_BPTT = tampung_hitung_manual(\"backward\",backward)\n",
    "\n",
    "        data_perhitungan_training_BPTT = pd.concat([data_perhitungan_new_training_BPTT, data_perhitungan_training_BPTT]).reset_index(drop = True) \n",
    "        # # hasil error yang dihasilkan backword\n",
    "        E = result[0]\n",
    "        print (\"E\",E)\n",
    "        # hasil turunan bobot dihasilkan backword\n",
    "        dE_dW = result[1]\n",
    "        w = dE_dW.shape\n",
    "        \n",
    "        # Annealing () Optimasi\n",
    "        adaptiveLearningRate = rate / (1 + (epoch/10))\n",
    "        lstm.W = lstm.W - adaptiveLearningRate * dE_dW\n",
    "        print (pd.DataFrame(list(W)))\n",
    "        r = [list(w)]\n",
    "        epochError += E\n",
    "    # for = [\"I\",\"Z\",\"C\",\"O\",\"F\",\"I_in\",\"C_bar\",\"h\",\"W\"]\n",
    "    # back = [\"dE_dW_t\", \"dE_dh_tminus1\", \"dE_dc_tminus1\", \"dE_do_t\", \"dE_dc_t\", \"dE_di_t\", \"dE_dcbar_t\", \"dE_df_t\",\"dE_dzcbar_t\",\"dE_dzi_t\",\"dE_dzf_t\",\"dE_dzo_t\",\"dE_dz_t\",\"dE_dI_t\",\"dE_dh_tplus1\", \"dE_dc_tplus1\"]\n",
    "    print('Epoch ' + str(epoch) + ' error: ' + str(epochError / counter))\n",
    "    print (view_hitung_manual(\"forward\",data_perhitungan_training,\"I\"))\n",
    "    print (view_hitung_manual(\"backward\",data_perhitungan_training_BPTT,\"dE_dc_tminus1\"))\n",
    "    data_perhitungan_training_BPTT.to_csv(\"perhitungan_bbpt.csv\")\n",
    "    data_perhitungan_training.to_csv(\"perhitungan_training.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forecast(forecastingData):\n",
    "    forward = forwardPass(forecastingData)\n",
    "    f_l = np.transpose(np.transpose(forward[0]))\n",
    "    f_z = np.transpose(np.transpose(forward[1]))\n",
    "    f_c = np.transpose(np.transpose(forward[2]))\n",
    "    f_o = np.transpose(np.transpose(forward[3]))\n",
    "    f_f = np.transpose(np.transpose(forward[4]))\n",
    "    f_i = np.transpose(np.transpose(forward[5]))\n",
    "    f_c_bar = np.transpose(np.transpose(forward[6]))\n",
    "    f_h = np.transpose(np.transpose(forward[7]))\n",
    "    f_W = np.transpose(np.transpose(forward[8]))\n",
    "    return (f_h[-1],f_l,f_z,f_c,f_o,f_f,f_i,f_c_bar,f_h,f_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[-0.02090026]\nError: 258.6407208763958\n----------------\n"
    }
   ],
   "source": [
    "forecastSequences = forecastSequenceProducer(testing_Data, sequenceLength)\n",
    "forecastError = 0.0\n",
    "forecastError_MSE = 0.0\n",
    "forecastError_MAPE = 0.0\n",
    "countForecasts = 0\n",
    "# Data Real\n",
    "labels = []\n",
    "\n",
    "# Data Preiksi\n",
    "forecasts = []\n",
    "max_ex = data[1]\n",
    "min_ex = data[2]\n",
    "\n",
    "data_full_testing_csv = {}\n",
    "data_perhitungan_testing = pd.DataFrame(data_full_testing_csv) # df \n",
    "\n",
    "for sequence in forecastSequences: \n",
    "    countForecasts += 1\n",
    "    forecast_w = forecast(sequence[:-1])\n",
    "    # print (\"squence predict \",R)\n",
    "    V_Predict = forecast_w[0]\n",
    "    print (V_Predict)\n",
    "    V_Predict *= max_ex[1:]\n",
    "    V_Predict += min_ex[1:]\n",
    "    # data_sequence_close_NT = sequence[:,2:]\n",
    "    # sequenceNT = denormal(sequenceLength,sequence,max_ex,min_ex)\n",
    "\n",
    "    # block proses\n",
    "    forward = [forecast_w[1],forecast_w[2],forecast_w[3],forecast_w[4],forecast_w[5],forecast_w[6],forecast_w[7],forecast_w[8],forecast_w[9]]\n",
    "    # melihat hasil perhitungan secara detail \n",
    "    data_perhitungan_new_testing = tampung_hitung_manual(\"forward\",forward)\n",
    "\n",
    "    data_perhitungan_testing = pd.concat([data_perhitungan_new_testing, data_perhitungan_testing]).reset_index(drop = True) \n",
    "    # ----------------------------------------------------------------------------------\n",
    "\n",
    "    label = sequence[-1,2:] * max_ex[1:]\n",
    "    label += min_ex[1:]\n",
    "\n",
    "    forecasts.append(V_Predict)\n",
    "\n",
    "    labels.append(label)\n",
    "\n",
    "    print('Error: ' + str(np.absolute( label[-1]-V_Predict[-1] )))\n",
    "\n",
    "    forecastError += np.absolute(label[-1]-V_Predict[-1])\n",
    "    \n",
    "    forecastError_MSE += (np.absolute(label[-1]-V_Predict[-1]))**2\n",
    "    \n",
    "    # print_sequence(sequence,max_ex,min_ex)\n",
    "\n",
    "    print ('----------------')\n",
    "\n",
    "data_perhitungan_testing.to_csv(\"datatata.csv\") # block preses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hasil Prediksi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Real  1\n[12000.]\nPrediksi  1\n[11741.35927912]\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# dari dataframe ke array \n",
    "# data prediksi \n",
    "forecasts = np.array(forecasts)\n",
    "\n",
    "# originalData = np.array(originalData) \n",
    "\n",
    "# data real belum \n",
    "labels = np.array(labels)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# memotongan data tiap hari ke 5 \n",
    "times = [i for i in range(forecasts.shape[0])]\n",
    "\n",
    "real = np.array(labels[:,-1])\n",
    "print (\"Real \", len(real))\n",
    "print (real)\n",
    "reali = real.tolist()\n",
    "\n",
    "prediksi = np.array(forecasts[:,-1])\n",
    "print (\"Prediksi \",len(prediksi)) \n",
    "print (prediksi)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluasi Acuracy, MAPE, MSE, DAN MAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "\n",
    "y_true = np.random.randn(100)\n",
    "y_pred = y_true * 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Average forecast error: (MAD) = 258.6407208763958\nAverage forecast error: (MSE) = 66895.02249546167\nAverage forecast error: (MAPE) = 2.1553393406366315 %\nAverage Secore Accuracy: 97.84466065936337 %\n"
    }
   ],
   "source": [
    "print('Average forecast error: (MAD) = ' + str(forecastError / countForecasts))\n",
    "print('Average forecast error: (MSE) = ' + str(forecastError_MSE / countForecasts))\n",
    "\n",
    "print ('Average forecast error: (MAPE) = ' + str(mean_absolute_percentage_error(reali, prediksi))+\" %\")\n",
    "print ('Average Secore Accuracy: ' + str(100 - mean_absolute_percentage_error(real, prediksi))+\" %\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisasi hasi perbandingan prediksi dan real "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab as pl\n",
    "pl.plot(times, forecasts, 'r')\n",
    "pl.plot(times, labels[:,-1], 'b')\n",
    "# pl.plot(x,y,'*k')\n",
    "pl.show()\n",
    ""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}